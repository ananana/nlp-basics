{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalization with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    " 'This is the first document.',\n",
    " 'This document is the second document.',\n",
    " 'And this is the third one.',\n",
    " 'Is this the first document?',\n",
    " '@user This one is a tweet #meta ;)' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'meta', 'one', 'second', 'the', 'third', 'this', 'tweet', 'user']\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0 1 0 1 0 0]\n",
      " [0 2 0 1 0 0 1 1 0 1 0 0]\n",
      " [1 0 0 1 0 1 0 1 1 1 0 0]\n",
      " [0 1 1 1 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 1 1 1 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True)\n",
    "tokenize_funct = tokenizer.tokenize\n",
    "word_blacklist = stopwords.words('english') + list(string.punctuation)\n",
    "vectorizer_tweet = CountVectorizer(tokenizer=tokenize_funct, stop_words=word_blacklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#meta', ';)', 'document', 'first', 'one', 'second', 'third', 'tweet']\n"
     ]
    }
   ],
   "source": [
    "X1 = vectorizer_tweet.fit_transform(corpus)\n",
    "print(vectorizer_tweet.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 0 0]\n",
      " [0 0 2 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 1 0]\n",
      " [0 0 1 1 0 0 0 0]\n",
      " [1 1 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this', 'document is', 'first document', 'is the', 'is this', 'is tweet', 'one is', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this one', 'this the', 'tweet meta', 'user this']\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'meta', 'one', 'second', 'third', 'tweet', 'user']\n"
     ]
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(stop_words=word_blacklist)\n",
    "X3 = vectorizer_tfidf.fit_transform(corpus)\n",
    "print(vectorizer_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63871058, 0.76944707, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.80130969, 0.        , 0.        , 0.        , 0.59824977,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.62791376, 0.        ,\n",
       "        0.77828292, 0.        , 0.        ],\n",
       "       [0.63871058, 0.76944707, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.52335825, 0.42224214, 0.        ,\n",
       "        0.        , 0.52335825, 0.52335825]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.63871058 0.76944707 0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "query = 'looking for the first document'\n",
    "Xquery = vectorizer_tfidf.transform([query])\n",
    "print(Xquery.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 - query similarity: 1.000000 ('This is the first document.')\n",
      "Doc 1 - query similarity: 0.511805 ('This document is the second document.')\n",
      "Doc 2 - query similarity: 0.000000 ('And this is the third one.')\n",
      "Doc 3 - query similarity: 1.000000 ('Is this the first document?')\n",
      "Doc 4 - query similarity: 0.000000 ('@user This one is a tweet #meta ;)')\n"
     ]
    }
   ],
   "source": [
    "for i, d1 in enumerate(X3):\n",
    "    print(\"Doc %d - query similarity: %f ('%s')\" % (i, cosine_similarity(d1,Xquery), corpus[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.48819503],\n",
       "       [1.        ],\n",
       "       [0.        ],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "dist = cdist(X3.toarray(), Xquery.toarray(), metric='cosine')\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [3],\n",
       "       [1],\n",
       "       [2],\n",
       "       [4]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_rank = np.argsort(dist, axis=0)\n",
    "similarity_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 'looking for the first document'...\n",
      "Ranked results:\n",
      "\t This is the first document. [0.]\n",
      "\t Is this the first document? [0.]\n",
      "\t This document is the second document. [0.48819503]\n",
      "\t And this is the third one. [1.]\n",
      "\t @user This one is a tweet #meta ;) [1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Searching for '%s'...\" % query)\n",
    "print(\"Ranked results:\")\n",
    "for i in similarity_rank.flatten():\n",
    "    print(\"\\t\", corpus[i], dist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
